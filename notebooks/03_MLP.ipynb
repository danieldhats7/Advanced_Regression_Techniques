{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.GetData import Data2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../in/train.csv'\n",
    "df = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = data.get_data2(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = dfx.drop('SalePrice', axis=1)\n",
    "y = dfx['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.c1 = nn.Linear(D_in, 256)\n",
    "        self.c2 = nn.ReLU()\n",
    "        self.c3 = nn.Dropout(p=0.1)\n",
    "        self.c4 = nn.Linear(256, 256)\n",
    "        self.c5 = nn.ReLU()\n",
    "        self.c6 = nn.Dropout(p=0.1)\n",
    "        self.c7 = nn.Linear(256, 128)\n",
    "        self.c8 = nn.ReLU()\n",
    "        self.c9 = nn.Dropout(p=0.1)\n",
    "        self.c10 = nn.Linear(128, 128)\n",
    "        self.c11 = nn.ReLU()\n",
    "        self.c12 = nn.Dropout(p=0.1)\n",
    "        self.c13 = nn.Linear(128, D_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.c1(X)\n",
    "        X = self.c2(X)\n",
    "        X = self.c3(X)\n",
    "        X = self.c4(X)\n",
    "        X = self.c5(X)\n",
    "        X = self.c6(X)\n",
    "        X = self.c7(X)\n",
    "        X = self.c8(X)\n",
    "        X = self.c9(X)\n",
    "        X = self.c10(X)\n",
    "        X = self.c11(X)\n",
    "        X = self.c12(X)\n",
    "        X = self.c13(X)\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(feature.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(feature.astype('float64').values)\n",
    "y = torch.from_numpy(y.astype('float64').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.type(torch.FloatTensor)\n",
    "y = y.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\Desktop\\practica\\Advanced_Regression_Techniques\\renv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 Loss 6590039314.28571\n",
      "Epoch 2/500 Loss 6605700800.00000\n",
      "Epoch 3/500 Loss 6610329252.57143\n",
      "Epoch 4/500 Loss 6611846715.42857\n",
      "Epoch 5/500 Loss 6609676189.25714\n",
      "Epoch 6/500 Loss 6606728225.52381\n",
      "Epoch 7/500 Loss 6604549958.53061\n",
      "Epoch 8/500 Loss 6601942918.85714\n",
      "Epoch 9/500 Loss 6601954423.87302\n",
      "Epoch 10/500 Loss 6600726581.02857\n",
      "Epoch 11/500 Loss 6599609688.10390\n",
      "Epoch 12/500 Loss 6601290823.61905\n",
      "Epoch 13/500 Loss 6603022106.72528\n",
      "Epoch 14/500 Loss 6604325304.16327\n",
      "Epoch 15/500 Loss 6606508708.57143\n",
      "Epoch 16/500 Loss 6606830986.28571\n",
      "Epoch 17/500 Loss 6607999749.37815\n",
      "Epoch 18/500 Loss 6608732616.12698\n",
      "Epoch 19/500 Loss 6608836046.91729\n",
      "Epoch 20/500 Loss 6608709104.45714\n",
      "Epoch 21/500 Loss 6608302066.93877\n",
      "Epoch 22/500 Loss 6608267303.06493\n",
      "Epoch 23/500 Loss 6609663151.70186\n",
      "Epoch 24/500 Loss 6610696965.33333\n",
      "Epoch 25/500 Loss 6610546548.29714\n",
      "Epoch 26/500 Loss 6611437485.71428\n",
      "Epoch 27/500 Loss 6612058898.28571\n",
      "Epoch 28/500 Loss 6612972912.97959\n",
      "Epoch 29/500 Loss 6612706714.48276\n",
      "Epoch 30/500 Loss 6611783714.74286\n",
      "Epoch 31/500 Loss 6610692253.49309\n",
      "Epoch 32/500 Loss 6610624544.57143\n",
      "Epoch 33/500 Loss 6610469326.68398\n",
      "Epoch 34/500 Loss 6610435227.42857\n",
      "Epoch 35/500 Loss 6609886674.54694\n",
      "Epoch 36/500 Loss 6609718618.92064\n",
      "Epoch 37/500 Loss 6609817110.23938\n",
      "Epoch 38/500 Loss 6609867733.17293\n",
      "Epoch 39/500 Loss 6609647924.04396\n",
      "Epoch 40/500 Loss 6609990726.85714\n",
      "Epoch 41/500 Loss 6609809623.41463\n",
      "Epoch 42/500 Loss 6609741450.88435\n",
      "Epoch 43/500 Loss 6609928649.99336\n",
      "Epoch 44/500 Loss 6609752780.05195\n",
      "Epoch 45/500 Loss 6609359547.32698\n",
      "Epoch 46/500 Loss 6608756711.75155\n",
      "Epoch 47/500 Loss 6608505248.68085\n",
      "Epoch 48/500 Loss 6608362961.90476\n",
      "Epoch 49/500 Loss 6608230010.02915\n",
      "Epoch 50/500 Loss 6607684277.76000\n",
      "Epoch 51/500 Loss 6607317767.52941\n",
      "Epoch 52/500 Loss 6607407294.24176\n",
      "Epoch 53/500 Loss 6607406061.02426\n",
      "Epoch 54/500 Loss 6607655133.12169\n",
      "Epoch 55/500 Loss 6608047095.02338\n",
      "Epoch 56/500 Loss 6608520441.79592\n",
      "Epoch 57/500 Loss 6608864281.34336\n",
      "Epoch 58/500 Loss 6609054332.53202\n",
      "Epoch 59/500 Loss 6609447532.16465\n",
      "Epoch 60/500 Loss 6609427165.56190\n",
      "Epoch 61/500 Loss 6609683242.26698\n",
      "Epoch 62/500 Loss 6609650873.51152\n",
      "Epoch 63/500 Loss 6609433634.53968\n",
      "Epoch 64/500 Loss 6609096128.28571\n",
      "Epoch 65/500 Loss 6609000156.27253\n",
      "Epoch 66/500 Loss 6609160404.50216\n",
      "Epoch 67/500 Loss 6608992544.47761\n",
      "Epoch 68/500 Loss 6608782181.37815\n",
      "Epoch 69/500 Loss 6608622603.92547\n",
      "Epoch 70/500 Loss 6608281735.05306\n",
      "Epoch 71/500 Loss 6608156347.75050\n",
      "Epoch 72/500 Loss 6608064345.14286\n",
      "Epoch 73/500 Loss 6608369836.58708\n",
      "Epoch 74/500 Loss 6608671025.66795\n",
      "Epoch 75/500 Loss 6608668596.17524\n",
      "Epoch 76/500 Loss 6608519224.06015\n",
      "Epoch 77/500 Loss 6608821807.73284\n",
      "Epoch 78/500 Loss 6608822394.13919\n",
      "Epoch 79/500 Loss 6609077437.10669\n",
      "Epoch 80/500 Loss 6609248949.25714\n",
      "Epoch 81/500 Loss 6609024668.89594\n",
      "Epoch 82/500 Loss 6608702123.48432\n",
      "Epoch 83/500 Loss 6608585054.07229\n",
      "Epoch 84/500 Loss 6608515137.52381\n",
      "Epoch 85/500 Loss 6608365085.47227\n",
      "Epoch 86/500 Loss 6608294471.22924\n",
      "Epoch 87/500 Loss 6608559271.09360\n",
      "Epoch 88/500 Loss 6608714052.36364\n",
      "Epoch 89/500 Loss 6608771058.64526\n",
      "Epoch 90/500 Loss 6608772527.33968\n",
      "Epoch 91/500 Loss 6608639976.08791\n",
      "Epoch 92/500 Loss 6608528644.57143\n",
      "Epoch 93/500 Loss 6608298738.03994\n",
      "Epoch 94/500 Loss 6608456832.58359\n",
      "Epoch 95/500 Loss 6608762641.90075\n",
      "Epoch 96/500 Loss 6608920969.71429\n",
      "Epoch 97/500 Loss 6608913337.68483\n",
      "Epoch 98/500 Loss 6608876703.53353\n",
      "Epoch 99/500 Loss 6608665687.36508\n",
      "Epoch 100/500 Loss 6608840168.04572\n",
      "Epoch 101/500 Loss 6608936667.97171\n",
      "Epoch 102/500 Loss 6608981876.70588\n",
      "Epoch 103/500 Loss 6609087741.33703\n",
      "Epoch 104/500 Loss 6609368155.25275\n",
      "Epoch 105/500 Loss 6609402474.75374\n",
      "Epoch 106/500 Loss 6609532860.54986\n",
      "Epoch 107/500 Loss 6609372610.99065\n",
      "Epoch 108/500 Loss 6609314610.62434\n",
      "Epoch 109/500 Loss 6609229464.15727\n",
      "Epoch 110/500 Loss 6609171711.83377\n",
      "Epoch 111/500 Loss 6609253914.85199\n",
      "Epoch 112/500 Loss 6609254167.02041\n",
      "Epoch 113/500 Loss 6609320485.05689\n",
      "Epoch 114/500 Loss 6609245996.75188\n",
      "Epoch 115/500 Loss 6609097473.11304\n",
      "Epoch 116/500 Loss 6608977348.25616\n",
      "Epoch 117/500 Loss 6609029473.36752\n",
      "Epoch 118/500 Loss 6609014646.54722\n",
      "Epoch 119/500 Loss 6608865980.23529\n",
      "Epoch 120/500 Loss 6608745034.20952\n",
      "Epoch 121/500 Loss 6608661737.18064\n",
      "Epoch 122/500 Loss 6608843841.19906\n",
      "Epoch 123/500 Loss 6608887410.91754\n",
      "Epoch 124/500 Loss 6608907488.29493\n",
      "Epoch 125/500 Loss 6608862706.68800\n",
      "Epoch 126/500 Loss 6608688604.73469\n",
      "Epoch 127/500 Loss 6608570814.20022\n",
      "Epoch 128/500 Loss 6608479275.00000\n",
      "Epoch 129/500 Loss 6608504029.83832\n",
      "Epoch 130/500 Loss 6608518674.14505\n",
      "Epoch 131/500 Loss 6608587242.64340\n",
      "Epoch 132/500 Loss 6608446113.93939\n",
      "Epoch 133/500 Loss 6608297901.64554\n",
      "Epoch 134/500 Loss 6608209553.60341\n",
      "Epoch 135/500 Loss 6608119740.95238\n",
      "Epoch 136/500 Loss 6608091801.14286\n",
      "Epoch 137/500 Loss 6608178786.90302\n",
      "Epoch 138/500 Loss 6608126005.13457\n",
      "Epoch 139/500 Loss 6608119082.22816\n",
      "Epoch 140/500 Loss 6607926108.08163\n",
      "Epoch 141/500 Loss 6607668568.05674\n",
      "Epoch 142/500 Loss 6607584228.57143\n",
      "Epoch 143/500 Loss 6607475653.81818\n",
      "Epoch 144/500 Loss 6607477629.84127\n",
      "Epoch 145/500 Loss 6607509533.38325\n",
      "Epoch 146/500 Loss 6607314844.18004\n",
      "Epoch 147/500 Loss 6607281048.13217\n",
      "Epoch 148/500 Loss 6607272528.18533\n",
      "Epoch 149/500 Loss 6607240248.82071\n",
      "Epoch 150/500 Loss 6607408420.93714\n",
      "Epoch 151/500 Loss 6607466829.62346\n",
      "Epoch 152/500 Loss 6607414923.18797\n",
      "Epoch 153/500 Loss 6607392797.75910\n",
      "Epoch 154/500 Loss 6607245202.16698\n",
      "Epoch 155/500 Loss 6607342793.37880\n",
      "Epoch 156/500 Loss 6607352135.15018\n",
      "Epoch 157/500 Loss 6607231024.80073\n",
      "Epoch 158/500 Loss 6607073126.19168\n",
      "Epoch 159/500 Loss 6607090871.89218\n",
      "Epoch 160/500 Loss 6607187434.17143\n",
      "Epoch 161/500 Loss 6607253480.71695\n",
      "Epoch 162/500 Loss 6607144124.83951\n",
      "Epoch 163/500 Loss 6606909687.36196\n",
      "Epoch 164/500 Loss 6606799528.13937\n",
      "Epoch 165/500 Loss 6606738561.66234\n",
      "Epoch 166/500 Loss 6606606728.15146\n",
      "Epoch 167/500 Loss 6606548168.59538\n",
      "Epoch 168/500 Loss 6606601312.21769\n",
      "Epoch 169/500 Loss 6606661983.86475\n",
      "Epoch 170/500 Loss 6606607781.00168\n",
      "Epoch 171/500 Loss 6606409949.99499\n",
      "Epoch 172/500 Loss 6606164129.70100\n",
      "Epoch 173/500 Loss 6606061947.13790\n",
      "Epoch 174/500 Loss 6606163830.22660\n",
      "Epoch 175/500 Loss 6606203668.37551\n",
      "Epoch 176/500 Loss 6606131412.67532\n",
      "Epoch 177/500 Loss 6606100375.96772\n",
      "Epoch 178/500 Loss 6606042858.52969\n",
      "Epoch 179/500 Loss 6606089492.53312\n",
      "Epoch 180/500 Loss 6606128272.76190\n",
      "Epoch 181/500 Loss 6606046406.92028\n",
      "Epoch 182/500 Loss 6606080093.94034\n",
      "Epoch 183/500 Loss 6606137461.40827\n",
      "Epoch 184/500 Loss 6606099285.16770\n",
      "Epoch 185/500 Loss 6606132362.67490\n",
      "Epoch 186/500 Loss 6606052439.39785\n",
      "Epoch 187/500 Loss 6606100394.82964\n",
      "Epoch 188/500 Loss 6606128519.48936\n",
      "Epoch 189/500 Loss 6606209152.58050\n",
      "Epoch 190/500 Loss 6606323684.18646\n",
      "Epoch 191/500 Loss 6606359774.01346\n",
      "Epoch 192/500 Loss 6606350891.14286\n",
      "Epoch 193/500 Loss 6606286508.90896\n",
      "Epoch 194/500 Loss 6606257503.67010\n",
      "Epoch 195/500 Loss 6606251073.92234\n",
      "Epoch 196/500 Loss 6606247057.63265\n",
      "Epoch 197/500 Loss 6606085685.55765\n",
      "Epoch 198/500 Loss 6606067514.45887\n",
      "Epoch 199/500 Loss 6605953023.72434\n",
      "Epoch 200/500 Loss 6605797865.41714\n",
      "Epoch 201/500 Loss 6605778211.93461\n",
      "Epoch 202/500 Loss 6605986295.40028\n",
      "Epoch 203/500 Loss 6606052756.89796\n",
      "Epoch 204/500 Loss 6605997790.74510\n",
      "Epoch 205/500 Loss 6605883301.28502\n",
      "Epoch 206/500 Loss 6605839199.77809\n",
      "Epoch 207/500 Loss 6605766526.58661\n",
      "Epoch 208/500 Loss 6605777609.75824\n",
      "Epoch 209/500 Loss 6605798125.71429\n",
      "Epoch 210/500 Loss 6605780424.01088\n",
      "Epoch 211/500 Loss 6605792036.83142\n",
      "Epoch 212/500 Loss 6605748677.08895\n",
      "Epoch 213/500 Loss 6605706021.68746\n",
      "Epoch 214/500 Loss 6605683222.30173\n",
      "Epoch 215/500 Loss 6605706686.93688\n",
      "Epoch 216/500 Loss 6605734143.06878\n",
      "Epoch 217/500 Loss 6605653258.36471\n",
      "Epoch 218/500 Loss 6605672781.92398\n",
      "Epoch 219/500 Loss 6605701384.26615\n",
      "Epoch 220/500 Loss 6605715106.82597\n",
      "Epoch 221/500 Loss 6605722280.87395\n",
      "Epoch 222/500 Loss 6605805523.43887\n",
      "Epoch 223/500 Loss 6605782448.70724\n",
      "Epoch 224/500 Loss 6605789923.34694\n",
      "Epoch 225/500 Loss 6605864900.42921\n",
      "Epoch 226/500 Loss 6605988778.31606\n",
      "Epoch 227/500 Loss 6605999377.31907\n",
      "Epoch 228/500 Loss 6605966092.27068\n",
      "Epoch 229/500 Loss 6605898356.50156\n",
      "Epoch 230/500 Loss 6605825910.69814\n",
      "Epoch 231/500 Loss 6605727565.65492\n",
      "Epoch 232/500 Loss 6605728986.56158\n",
      "Epoch 233/500 Loss 6605784849.50092\n",
      "Epoch 234/500 Loss 6605751733.21612\n",
      "Epoch 235/500 Loss 6605765785.52219\n",
      "Epoch 236/500 Loss 6605841631.69007\n",
      "Epoch 237/500 Loss 6605835615.59494\n",
      "Epoch 238/500 Loss 6605879466.64106\n",
      "Epoch 239/500 Loss 6605947796.65750\n",
      "Epoch 240/500 Loss 6606010322.81905\n",
      "Epoch 241/500 Loss 6605974330.34736\n",
      "Epoch 242/500 Loss 6605992183.91499\n",
      "Epoch 243/500 Loss 6605927690.00823\n",
      "Epoch 244/500 Loss 6605908612.87119\n",
      "Epoch 245/500 Loss 6605800958.43265\n",
      "Epoch 246/500 Loss 6605734607.90708\n",
      "Epoch 247/500 Loss 6605796737.18450\n",
      "Epoch 248/500 Loss 6605823396.94009\n",
      "Epoch 249/500 Loss 6605830206.34768\n",
      "Epoch 250/500 Loss 6605798643.34628\n",
      "Epoch 251/500 Loss 6605756080.51907\n",
      "Epoch 252/500 Loss 6605742100.24490\n",
      "Epoch 253/500 Loss 6605813041.65330\n",
      "Epoch 254/500 Loss 6605728844.52643\n",
      "Epoch 255/500 Loss 6605745177.74342\n",
      "Epoch 256/500 Loss 6605868580.50000\n",
      "Epoch 257/500 Loss 6606034433.35186\n",
      "Epoch 258/500 Loss 6606095123.63234\n",
      "Epoch 259/500 Loss 6606036806.24821\n",
      "Epoch 260/500 Loss 6605943751.38461\n",
      "Epoch 261/500 Loss 6605938083.31034\n",
      "Epoch 262/500 Loss 6605907735.79935\n",
      "Epoch 263/500 Loss 6605971097.86420\n",
      "Epoch 264/500 Loss 6605912954.94372\n",
      "Epoch 265/500 Loss 6605903147.81671\n",
      "Epoch 266/500 Loss 6605902592.61869\n",
      "Epoch 267/500 Loss 6605917041.07009\n",
      "Epoch 268/500 Loss 6605818595.27505\n",
      "Epoch 269/500 Loss 6605707484.04036\n",
      "Epoch 270/500 Loss 6605649195.54709\n",
      "Epoch 271/500 Loss 6605604438.16552\n",
      "Epoch 272/500 Loss 6605512553.47899\n",
      "Epoch 273/500 Loss 6605510209.97593\n",
      "Epoch 274/500 Loss 6605495361.20125\n",
      "Epoch 275/500 Loss 6605490475.55325\n",
      "Epoch 276/500 Loss 6605425720.11594\n",
      "Epoch 277/500 Loss 6605428346.05879\n",
      "Epoch 278/500 Loss 6605370313.47174\n",
      "Epoch 279/500 Loss 6605324414.55812\n",
      "Epoch 280/500 Loss 6605286542.82449\n",
      "Epoch 281/500 Loss 6605282212.57143\n",
      "Epoch 282/500 Loss 6605240435.61500\n",
      "Epoch 283/500 Loss 6605212415.54770\n",
      "Epoch 284/500 Loss 6605206336.06439\n",
      "Epoch 285/500 Loss 6605243779.07970\n",
      "Epoch 286/500 Loss 6605199075.03696\n",
      "Epoch 287/500 Loss 6605208318.15232\n",
      "Epoch 288/500 Loss 6605195123.61905\n",
      "Epoch 289/500 Loss 6605218018.95798\n",
      "Epoch 290/500 Loss 6605208839.75566\n",
      "Epoch 291/500 Loss 6605245418.32106\n",
      "Epoch 292/500 Loss 6605286958.90411\n",
      "Epoch 293/500 Loss 6605370318.75963\n",
      "Epoch 294/500 Loss 6605454169.50049\n",
      "Epoch 295/500 Loss 6605472618.67700\n",
      "Epoch 296/500 Loss 6605444322.03861\n",
      "Epoch 297/500 Loss 6605430079.47667\n",
      "Epoch 298/500 Loss 6605461258.86098\n",
      "Epoch 299/500 Loss 6605476785.04730\n",
      "Epoch 300/500 Loss 6605433956.02286\n",
      "Epoch 301/500 Loss 6605405266.31609\n",
      "Epoch 302/500 Loss 6605339525.87323\n",
      "Epoch 303/500 Loss 6605378255.90193\n",
      "Epoch 304/500 Loss 6605436284.45113\n",
      "Epoch 305/500 Loss 6605516818.40562\n",
      "Epoch 306/500 Loss 6605535285.48273\n",
      "Epoch 307/500 Loss 6605473035.85295\n",
      "Epoch 308/500 Loss 6605452845.89239\n",
      "Epoch 309/500 Loss 6605487357.57374\n",
      "Epoch 310/500 Loss 6605569845.61843\n",
      "Epoch 311/500 Loss 6605591896.60634\n",
      "Epoch 312/500 Loss 6605557363.04762\n",
      "Epoch 313/500 Loss 6605524447.22592\n",
      "Epoch 314/500 Loss 6605533589.95450\n",
      "Epoch 315/500 Loss 6605559700.89796\n",
      "Epoch 316/500 Loss 6605501845.69982\n",
      "Epoch 317/500 Loss 6605451775.48085\n",
      "Epoch 318/500 Loss 6605461531.42857\n",
      "Epoch 319/500 Loss 6605435148.20958\n",
      "Epoch 320/500 Loss 6605473733.77143\n",
      "Epoch 321/500 Loss 6605472366.45483\n",
      "Epoch 322/500 Loss 6605450372.82697\n",
      "Epoch 323/500 Loss 6605441073.98850\n",
      "Epoch 324/500 Loss 6605456921.11464\n",
      "Epoch 325/500 Loss 6605462269.58066\n",
      "Epoch 326/500 Loss 6605406928.15425\n",
      "Epoch 327/500 Loss 6605403852.16252\n",
      "Epoch 328/500 Loss 6605352657.56098\n",
      "Epoch 329/500 Loss 6605344614.65567\n",
      "Epoch 330/500 Loss 6605299495.95152\n",
      "Epoch 331/500 Loss 6605219051.39404\n",
      "Epoch 332/500 Loss 6605134533.56282\n",
      "Epoch 333/500 Loss 6605077105.83269\n",
      "Epoch 334/500 Loss 6605087121.02652\n",
      "Epoch 335/500 Loss 6605159826.23113\n",
      "Epoch 336/500 Loss 6605226314.93877\n",
      "Epoch 337/500 Loss 6605197086.00593\n",
      "Epoch 338/500 Loss 6605193241.15638\n",
      "Epoch 339/500 Loss 6605178438.06827\n",
      "Epoch 340/500 Loss 6605111761.21008\n",
      "Epoch 341/500 Loss 6605076887.64809\n",
      "Epoch 342/500 Loss 6605094367.06433\n",
      "Epoch 343/500 Loss 6604985268.40483\n",
      "Epoch 344/500 Loss 6604955120.05315\n",
      "Epoch 345/500 Loss 6604945108.53830\n",
      "Epoch 346/500 Loss 6604901669.83980\n",
      "Epoch 347/500 Loss 6604851404.04117\n",
      "Epoch 348/500 Loss 6604857143.38259\n",
      "Epoch 349/500 Loss 6604837049.16250\n",
      "Epoch 350/500 Loss 6604877654.83102\n",
      "Epoch 351/500 Loss 6604852923.75417\n",
      "Epoch 352/500 Loss 6604819721.35065\n",
      "Epoch 353/500 Loss 6604820367.07406\n",
      "Epoch 354/500 Loss 6604850028.37127\n",
      "Epoch 355/500 Loss 6604859680.75976\n",
      "Epoch 356/500 Loss 6604941247.69181\n",
      "Epoch 357/500 Loss 6604965183.92317\n",
      "Epoch 358/500 Loss 6604945482.67518\n",
      "Epoch 359/500 Loss 6604928044.10983\n",
      "Epoch 360/500 Loss 6604929873.98095\n",
      "Epoch 361/500 Loss 6604857098.73842\n",
      "Epoch 362/500 Loss 6604727588.77348\n",
      "Epoch 363/500 Loss 6604648076.54309\n",
      "Epoch 364/500 Loss 6604555892.24490\n",
      "Epoch 365/500 Loss 6604504487.92798\n",
      "Epoch 366/500 Loss 6604506588.22795\n",
      "Epoch 367/500 Loss 6604487184.04360\n",
      "Epoch 368/500 Loss 6604460327.55279\n",
      "Epoch 369/500 Loss 6604452511.26907\n",
      "Epoch 370/500 Loss 6604403363.03938\n",
      "Epoch 371/500 Loss 6604337441.76203\n",
      "Epoch 372/500 Loss 6604338929.00768\n",
      "Epoch 373/500 Loss 6604272819.57258\n",
      "Epoch 374/500 Loss 6604281787.30634\n",
      "Epoch 375/500 Loss 6604315421.11086\n",
      "Epoch 376/500 Loss 6604363748.23100\n",
      "Epoch 377/500 Loss 6604406000.91550\n",
      "Epoch 378/500 Loss 6604420802.80574\n",
      "Epoch 379/500 Loss 6604362774.24199\n",
      "Epoch 380/500 Loss 6604309540.52331\n",
      "Epoch 381/500 Loss 6604296549.60330\n",
      "Epoch 382/500 Loss 6604308830.25280\n",
      "Epoch 383/500 Loss 6604353783.26296\n",
      "Epoch 384/500 Loss 6604421572.52381\n",
      "Epoch 385/500 Loss 6604431249.19332\n",
      "Epoch 386/500 Loss 6604432545.01851\n",
      "Epoch 387/500 Loss 6604467417.49133\n",
      "Epoch 388/500 Loss 6604541911.70545\n",
      "Epoch 389/500 Loss 6604572298.43555\n",
      "Epoch 390/500 Loss 6604552992.58608\n",
      "Epoch 391/500 Loss 6604533549.97150\n",
      "Epoch 392/500 Loss 6604545158.01749\n",
      "Epoch 393/500 Loss 6604546894.30753\n",
      "Epoch 394/500 Loss 6604517568.27846\n",
      "Epoch 395/500 Loss 6604548395.46908\n",
      "Epoch 396/500 Loss 6604555607.22655\n",
      "Epoch 397/500 Loss 6604524839.38107\n",
      "Epoch 398/500 Loss 6604489279.26490\n",
      "Epoch 399/500 Loss 6604426251.77802\n",
      "Epoch 400/500 Loss 6604393595.01714\n",
      "Epoch 401/500 Loss 6604413816.79516\n",
      "Epoch 402/500 Loss 6604410537.62047\n",
      "Epoch 403/500 Loss 6604347249.20808\n",
      "Epoch 404/500 Loss 6604319717.61245\n",
      "Epoch 405/500 Loss 6604268743.24656\n",
      "Epoch 406/500 Loss 6604254370.81492\n",
      "Epoch 407/500 Loss 6604265717.26220\n",
      "Epoch 408/500 Loss 6604210084.88515\n",
      "Epoch 409/500 Loss 6604198109.88753\n",
      "Epoch 410/500 Loss 6604202941.85923\n",
      "Epoch 411/500 Loss 6604198835.96524\n",
      "Epoch 412/500 Loss 6604213705.80860\n",
      "Epoch 413/500 Loss 6604251719.94742\n",
      "Epoch 414/500 Loss 6604199489.23672\n",
      "Epoch 415/500 Loss 6604222545.55869\n",
      "Epoch 416/500 Loss 6604204376.92308\n",
      "Epoch 417/500 Loss 6604157068.80439\n",
      "Epoch 418/500 Loss 6604102560.76555\n",
      "Epoch 419/500 Loss 6604061517.28878\n",
      "Epoch 420/500 Loss 6604010497.26258\n",
      "Epoch 421/500 Loss 6603922816.39091\n",
      "Epoch 422/500 Loss 6603879406.66757\n",
      "Epoch 423/500 Loss 6603876229.83587\n",
      "Epoch 424/500 Loss 6603823239.41779\n",
      "Epoch 425/500 Loss 6603760304.83361\n",
      "Epoch 426/500 Loss 6603749815.88732\n",
      "Epoch 427/500 Loss 6603766589.49481\n",
      "Epoch 428/500 Loss 6603774286.44059\n",
      "Epoch 429/500 Loss 6603787318.04729\n",
      "Epoch 430/500 Loss 6603817394.68970\n",
      "Epoch 431/500 Loss 6603756204.16573\n",
      "Epoch 432/500 Loss 6603729950.89947\n",
      "Epoch 433/500 Loss 6603737396.36556\n",
      "Epoch 434/500 Loss 6603758872.60566\n",
      "Epoch 435/500 Loss 6603716279.36158\n",
      "Epoch 436/500 Loss 6603710129.74050\n",
      "Epoch 437/500 Loss 6603681641.94835\n",
      "Epoch 438/500 Loss 6603675900.15917\n",
      "Epoch 439/500 Loss 6603662746.69964\n",
      "Epoch 440/500 Loss 6603674195.11688\n",
      "Epoch 441/500 Loss 6603682127.69420\n",
      "Epoch 442/500 Loss 6603676051.52683\n",
      "Epoch 443/500 Loss 6603614634.76298\n",
      "Epoch 444/500 Loss 6603547327.42342\n",
      "Epoch 445/500 Loss 6603486307.60578\n",
      "Epoch 446/500 Loss 6603460002.11147\n",
      "Epoch 447/500 Loss 6603450753.10451\n",
      "Epoch 448/500 Loss 6603479181.22449\n",
      "Epoch 449/500 Loss 6603528376.56761\n",
      "Epoch 450/500 Loss 6603557183.47175\n",
      "Epoch 451/500 Loss 6603549788.84764\n",
      "Epoch 452/500 Loss 6603488863.71681\n",
      "Epoch 453/500 Loss 6603448927.42479\n",
      "Epoch 454/500 Loss 6603444577.95343\n",
      "Epoch 455/500 Loss 6603458514.58713\n",
      "Epoch 456/500 Loss 6603465171.32832\n",
      "Epoch 457/500 Loss 6603433882.68834\n",
      "Epoch 458/500 Loss 6603447168.63880\n",
      "Epoch 459/500 Loss 6603473308.88266\n",
      "Epoch 460/500 Loss 6603539890.96149\n",
      "Epoch 461/500 Loss 6603550607.03316\n",
      "Epoch 462/500 Loss 6603585543.20346\n",
      "Epoch 463/500 Loss 6603633511.47424\n",
      "Epoch 464/500 Loss 6603649538.60098\n",
      "Epoch 465/500 Loss 6603624960.90445\n",
      "Epoch 466/500 Loss 6603596641.51073\n",
      "Epoch 467/500 Loss 6603527879.88987\n",
      "Epoch 468/500 Loss 6603444088.22466\n",
      "Epoch 469/500 Loss 6603398444.25221\n",
      "Epoch 470/500 Loss 6603343578.37811\n",
      "Epoch 471/500 Loss 6603331042.14498\n",
      "Epoch 472/500 Loss 6603290948.37772\n",
      "Epoch 473/500 Loss 6603267378.83661\n",
      "Epoch 474/500 Loss 6603217099.72755\n",
      "Epoch 475/500 Loss 6603210722.78135\n",
      "Epoch 476/500 Loss 6603189652.85954\n",
      "Epoch 477/500 Loss 6603189505.87841\n",
      "Epoch 478/500 Loss 6603194270.64196\n",
      "Epoch 479/500 Loss 6603186304.61080\n",
      "Epoch 480/500 Loss 6603187444.00000\n",
      "Epoch 481/500 Loss 6603153751.70300\n",
      "Epoch 482/500 Loss 6603127713.49852\n",
      "Epoch 483/500 Loss 6603121498.21710\n",
      "Epoch 484/500 Loss 6603131842.60685\n",
      "Epoch 485/500 Loss 6603120780.14021\n",
      "Epoch 486/500 Loss 6603130406.94180\n",
      "Epoch 487/500 Loss 6603140099.56703\n",
      "Epoch 488/500 Loss 6603139359.58782\n",
      "Epoch 489/500 Loss 6603133641.06807\n",
      "Epoch 490/500 Loss 6603142276.29154\n",
      "Epoch 491/500 Loss 6603125521.83881\n",
      "Epoch 492/500 Loss 6603112038.91289\n",
      "Epoch 493/500 Loss 6603106502.99160\n",
      "Epoch 494/500 Loss 6603164695.87507\n",
      "Epoch 495/500 Loss 6603170868.27128\n",
      "Epoch 496/500 Loss 6603121723.02304\n",
      "Epoch 497/500 Loss 6603114034.36850\n",
      "Epoch 498/500 Loss 6603099846.02180\n",
      "Epoch 499/500 Loss 6603104185.38563\n",
      "Epoch 500/500 Loss 6603119312.78629\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "batch_size = 100\n",
    "log_each = 1\n",
    "l = []\n",
    "model.train()\n",
    "batches = len(X) // batch_size\n",
    "for e in range(1, epochs + 1):\n",
    "    _l = []\n",
    "    for b in range(batches):\n",
    "        x_b = X[b*batch_size:(b+1)*batch_size]\n",
    "        y_b = y[b*batch_size:(b+1)*batch_size]\n",
    "\n",
    "        y_pred = model(x_b)\n",
    "        loss = criterion(y_pred, y_b)\n",
    "\n",
    "        _l.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    l.append(np.mean(_l))\n",
    "    if not e % log_each:\n",
    "        print(f'Epoch {e}/{epochs} Loss {np.mean(l):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_example = pd.read_csv('../in/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../in/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = data.get_data2(test.copy(), fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 207)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.from_numpy(test_df.astype('float64').values)\n",
    "X_test = X_test.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (c1): Linear(in_features=207, out_features=256, bias=True)\n",
       "  (c2): ReLU()\n",
       "  (c3): Dropout(p=0.1, inplace=False)\n",
       "  (c4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (c5): ReLU()\n",
       "  (c6): Dropout(p=0.1, inplace=False)\n",
       "  (c7): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (c8): ReLU()\n",
       "  (c9): Dropout(p=0.1, inplace=False)\n",
       "  (c10): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (c11): ReLU()\n",
       "  (c12): Dropout(p=0.1, inplace=False)\n",
       "  (c13): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = y_predict.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>174267.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>167103.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>177965.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>177942.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>175270.484375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SalePrice\n",
       "Id                 \n",
       "1461  174267.859375\n",
       "1462  167103.250000\n",
       "1463  177965.890625\n",
       "1464  177942.000000\n",
       "1465  175270.484375"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = pd.DataFrame(y_predict, columns=['SalePrice'])\n",
    "predict['Id'] = index=pred_example.Id\n",
    "predict = predict.set_index('Id')\n",
    "predict.to_csv('../out/MLP_500.csv')\n",
    "predict.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d1a0b3ba6560b31dd90a4016c7f41c1485500f8bcb36555c908d89fad0439e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('renv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
